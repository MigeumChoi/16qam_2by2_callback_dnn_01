{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN2MZJVIZxq9h0P6bBgK7bC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9xa0ISASmgIw","executionInfo":{"status":"ok","timestamp":1694578416794,"user_tz":-540,"elapsed":16121,"user":{"displayName":"최미금","userId":"03270121767541003919"}},"outputId":"c6f8171f-2836-4195-8d6f-11082c066b3b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3658\n","Epoch 1: val_loss improved from inf to 0.34838, saving model to hl5_0100.h5\n","1/1 [==============================] - 1s 997ms/step - loss: 0.3658 - val_loss: 0.3484\n","Epoch 2/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3618\n","Epoch 2: val_loss improved from 0.34838 to 0.34460, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.3618 - val_loss: 0.3446\n","Epoch 3/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3578\n","Epoch 3: val_loss improved from 0.34460 to 0.34032, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.3578 - val_loss: 0.3403\n","Epoch 4/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3532\n","Epoch 4: val_loss improved from 0.34032 to 0.33545, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.3532 - val_loss: 0.3354\n","Epoch 5/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3480"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 5: val_loss improved from 0.33545 to 0.32997, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.3480 - val_loss: 0.3300\n","Epoch 6/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3422\n","Epoch 6: val_loss improved from 0.32997 to 0.32378, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.3422 - val_loss: 0.3238\n","Epoch 7/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3356\n","Epoch 7: val_loss improved from 0.32378 to 0.31683, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.3356 - val_loss: 0.3168\n","Epoch 8/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3282\n","Epoch 8: val_loss improved from 0.31683 to 0.30907, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.3282 - val_loss: 0.3091\n","Epoch 9/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3199\n","Epoch 9: val_loss improved from 0.30907 to 0.30041, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.3199 - val_loss: 0.3004\n","Epoch 10/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3106\n","Epoch 10: val_loss improved from 0.30041 to 0.29079, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.3106 - val_loss: 0.2908\n","Epoch 11/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3002\n","Epoch 11: val_loss improved from 0.29079 to 0.28007, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.3002 - val_loss: 0.2801\n","Epoch 12/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2886\n","Epoch 12: val_loss improved from 0.28007 to 0.26817, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.2886 - val_loss: 0.2682\n","Epoch 13/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2757\n","Epoch 13: val_loss improved from 0.26817 to 0.25494, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.2757 - val_loss: 0.2549\n","Epoch 14/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2612\n","Epoch 14: val_loss improved from 0.25494 to 0.24032, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.2612 - val_loss: 0.2403\n","Epoch 15/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2451\n","Epoch 15: val_loss improved from 0.24032 to 0.22424, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.2451 - val_loss: 0.2242\n","Epoch 16/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2273\n","Epoch 16: val_loss improved from 0.22424 to 0.20662, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.2273 - val_loss: 0.2066\n","Epoch 17/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2076\n","Epoch 17: val_loss improved from 0.20662 to 0.18742, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.2076 - val_loss: 0.1874\n","Epoch 18/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1860\n","Epoch 18: val_loss improved from 0.18742 to 0.16673, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.1860 - val_loss: 0.1667\n","Epoch 19/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1626\n","Epoch 19: val_loss improved from 0.16673 to 0.14475, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1626 - val_loss: 0.1448\n","Epoch 20/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1373\n","Epoch 20: val_loss improved from 0.14475 to 0.12196, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.1373 - val_loss: 0.1220\n","Epoch 21/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1108\n","Epoch 21: val_loss improved from 0.12196 to 0.09960, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.1108 - val_loss: 0.0996\n","Epoch 22/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0840\n","Epoch 22: val_loss improved from 0.09960 to 0.08055, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0840 - val_loss: 0.0805\n","Epoch 23/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0599\n","Epoch 23: val_loss improved from 0.08055 to 0.06927, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0599 - val_loss: 0.0693\n","Epoch 24/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0434\n","Epoch 24: val_loss did not improve from 0.06927\n","1/1 [==============================] - 0s 31ms/step - loss: 0.0434 - val_loss: 0.0704\n","Epoch 25/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0395\n","Epoch 25: val_loss did not improve from 0.06927\n","1/1 [==============================] - 0s 30ms/step - loss: 0.0395 - val_loss: 0.0843\n","Epoch 26/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0494\n","Epoch 26: val_loss did not improve from 0.06927\n","1/1 [==============================] - 0s 32ms/step - loss: 0.0494 - val_loss: 0.1022\n","Epoch 27/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0648\n","Epoch 27: val_loss did not improve from 0.06927\n","1/1 [==============================] - 0s 27ms/step - loss: 0.0648 - val_loss: 0.1108\n","Epoch 28/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0722\n","Epoch 28: val_loss did not improve from 0.06927\n","1/1 [==============================] - 0s 26ms/step - loss: 0.0722 - val_loss: 0.1102\n","Epoch 29/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0710\n","Epoch 29: val_loss did not improve from 0.06927\n","1/1 [==============================] - 0s 28ms/step - loss: 0.0710 - val_loss: 0.1022\n","Epoch 30/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0629\n","Epoch 30: val_loss did not improve from 0.06927\n","1/1 [==============================] - 0s 29ms/step - loss: 0.0629 - val_loss: 0.0905\n","Epoch 31/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0513\n","Epoch 31: val_loss did not improve from 0.06927\n","1/1 [==============================] - 0s 33ms/step - loss: 0.0513 - val_loss: 0.0787\n","Epoch 32/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0399\n","Epoch 32: val_loss did not improve from 0.06927\n","1/1 [==============================] - 0s 29ms/step - loss: 0.0399 - val_loss: 0.0694\n","Epoch 33/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0314\n","Epoch 33: val_loss improved from 0.06927 to 0.06363, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0314 - val_loss: 0.0636\n","Epoch 34/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0264\n","Epoch 34: val_loss improved from 0.06363 to 0.06093, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0264 - val_loss: 0.0609\n","Epoch 35/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0246\n","Epoch 35: val_loss improved from 0.06093 to 0.06051, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0246 - val_loss: 0.0605\n","Epoch 36/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0249\n","Epoch 36: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 30ms/step - loss: 0.0249 - val_loss: 0.0614\n","Epoch 37/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0264\n","Epoch 37: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0264 - val_loss: 0.0629\n","Epoch 38/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0282\n","Epoch 38: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 37ms/step - loss: 0.0282 - val_loss: 0.0644\n","Epoch 39/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 39: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 36ms/step - loss: 0.0298 - val_loss: 0.0657\n","Epoch 40/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0308\n","Epoch 40: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 37ms/step - loss: 0.0308 - val_loss: 0.0665\n","Epoch 41/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0311\n","Epoch 41: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0311 - val_loss: 0.0668\n","Epoch 42/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0307\n","Epoch 42: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0307 - val_loss: 0.0668\n","Epoch 43/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 43: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0297 - val_loss: 0.0664\n","Epoch 44/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0282\n","Epoch 44: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0282 - val_loss: 0.0660\n","Epoch 45/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0265\n","Epoch 45: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 32ms/step - loss: 0.0265 - val_loss: 0.0656\n","Epoch 46/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0247\n","Epoch 46: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0247 - val_loss: 0.0654\n","Epoch 47/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0232\n","Epoch 47: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 42ms/step - loss: 0.0232 - val_loss: 0.0656\n","Epoch 48/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0220\n","Epoch 48: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0220 - val_loss: 0.0661\n","Epoch 49/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0212\n","Epoch 49: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0212 - val_loss: 0.0669\n","Epoch 50/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0209\n","Epoch 50: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0209 - val_loss: 0.0679\n","Epoch 51/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0209\n","Epoch 51: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0209 - val_loss: 0.0690\n","Epoch 52/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0211\n","Epoch 52: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 40ms/step - loss: 0.0211 - val_loss: 0.0700\n","Epoch 53/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0213\n","Epoch 53: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 33ms/step - loss: 0.0213 - val_loss: 0.0707\n","Epoch 54/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0213\n","Epoch 54: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 35ms/step - loss: 0.0213 - val_loss: 0.0710\n","Epoch 55/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0211\n","Epoch 55: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0211 - val_loss: 0.0711\n","Epoch 56/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0206\n","Epoch 56: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 42ms/step - loss: 0.0206 - val_loss: 0.0709\n","Epoch 57/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0199\n","Epoch 57: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 34ms/step - loss: 0.0199 - val_loss: 0.0705\n","Epoch 58/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0191\n","Epoch 58: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 34ms/step - loss: 0.0191 - val_loss: 0.0701\n","Epoch 59/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0183\n","Epoch 59: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 32ms/step - loss: 0.0183 - val_loss: 0.0698\n","Epoch 60/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0177\n","Epoch 60: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0177 - val_loss: 0.0697\n","Epoch 61/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0172\n","Epoch 61: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0172 - val_loss: 0.0697\n","Epoch 62/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0168\n","Epoch 62: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 36ms/step - loss: 0.0168 - val_loss: 0.0698\n","Epoch 63/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0166\n","Epoch 63: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0166 - val_loss: 0.0701\n","Epoch 64/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0165\n","Epoch 64: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0165 - val_loss: 0.0704\n","Epoch 65/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0165\n","Epoch 65: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0165 - val_loss: 0.0707\n","Epoch 66/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0164\n","Epoch 66: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0164 - val_loss: 0.0711\n","Epoch 67/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0162\n","Epoch 67: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 36ms/step - loss: 0.0162 - val_loss: 0.0714\n","Epoch 68/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0161\n","Epoch 68: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 33ms/step - loss: 0.0161 - val_loss: 0.0718\n","Epoch 69/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0158\n","Epoch 69: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 39ms/step - loss: 0.0158 - val_loss: 0.0721\n","Epoch 70/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0156\n","Epoch 70: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 38ms/step - loss: 0.0156 - val_loss: 0.0724\n","Epoch 71/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0153\n","Epoch 71: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 41ms/step - loss: 0.0153 - val_loss: 0.0727\n","Epoch 72/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0151\n","Epoch 72: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 32ms/step - loss: 0.0151 - val_loss: 0.0731\n","Epoch 73/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0148\n","Epoch 73: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 29ms/step - loss: 0.0148 - val_loss: 0.0735\n","Epoch 74/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0146\n","Epoch 74: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 29ms/step - loss: 0.0146 - val_loss: 0.0738\n","Epoch 75/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0144\n","Epoch 75: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 29ms/step - loss: 0.0144 - val_loss: 0.0742\n","Epoch 76/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0143\n","Epoch 76: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 33ms/step - loss: 0.0143 - val_loss: 0.0746\n","Epoch 77/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0141\n","Epoch 77: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 27ms/step - loss: 0.0141 - val_loss: 0.0749\n","Epoch 78/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0140\n","Epoch 78: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 30ms/step - loss: 0.0140 - val_loss: 0.0753\n","Epoch 79/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0139\n","Epoch 79: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 29ms/step - loss: 0.0139 - val_loss: 0.0756\n","Epoch 80/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0138\n","Epoch 80: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 31ms/step - loss: 0.0138 - val_loss: 0.0758\n","Epoch 81/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0136\n","Epoch 81: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 30ms/step - loss: 0.0136 - val_loss: 0.0760\n","Epoch 82/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0135\n","Epoch 82: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 31ms/step - loss: 0.0135 - val_loss: 0.0762\n","Epoch 83/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0133\n","Epoch 83: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 30ms/step - loss: 0.0133 - val_loss: 0.0764\n","Epoch 84/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0131\n","Epoch 84: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 32ms/step - loss: 0.0131 - val_loss: 0.0766\n","Epoch 85/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0130\n","Epoch 85: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 26ms/step - loss: 0.0130 - val_loss: 0.0768\n","Epoch 86/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0128\n","Epoch 86: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0128 - val_loss: 0.0770\n","Epoch 87/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0127\n","Epoch 87: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 27ms/step - loss: 0.0127 - val_loss: 0.0772\n","Epoch 88/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0126\n","Epoch 88: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 25ms/step - loss: 0.0126 - val_loss: 0.0775\n","Epoch 89/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0125\n","Epoch 89: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 26ms/step - loss: 0.0125 - val_loss: 0.0777\n","Epoch 90/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0124\n","Epoch 90: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 33ms/step - loss: 0.0124 - val_loss: 0.0780\n","Epoch 91/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0123\n","Epoch 91: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 36ms/step - loss: 0.0123 - val_loss: 0.0783\n","Epoch 92/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0122\n","Epoch 92: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 30ms/step - loss: 0.0122 - val_loss: 0.0785\n","Epoch 93/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0121\n","Epoch 93: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 30ms/step - loss: 0.0121 - val_loss: 0.0788\n","Epoch 94/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0120\n","Epoch 94: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 30ms/step - loss: 0.0120 - val_loss: 0.0791\n","Epoch 95/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0119\n","Epoch 95: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 33ms/step - loss: 0.0119 - val_loss: 0.0793\n","Epoch 96/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0118\n","Epoch 96: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 31ms/step - loss: 0.0118 - val_loss: 0.0795\n","Epoch 97/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0117\n","Epoch 97: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 32ms/step - loss: 0.0117 - val_loss: 0.0798\n","Epoch 98/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0116\n","Epoch 98: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 27ms/step - loss: 0.0116 - val_loss: 0.0800\n","Epoch 99/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0115\n","Epoch 99: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 27ms/step - loss: 0.0115 - val_loss: 0.0802\n","Epoch 100/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0114\n","Epoch 100: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 26ms/step - loss: 0.0114 - val_loss: 0.0804\n","Epoch 101/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0113\n","Epoch 101: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 32ms/step - loss: 0.0113 - val_loss: 0.0806\n","Epoch 102/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0112\n","Epoch 102: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 32ms/step - loss: 0.0112 - val_loss: 0.0808\n","Epoch 103/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0112\n","Epoch 103: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 30ms/step - loss: 0.0112 - val_loss: 0.0810\n","Epoch 104/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0111\n","Epoch 104: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 30ms/step - loss: 0.0111 - val_loss: 0.0812\n","Epoch 105/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0110\n","Epoch 105: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 25ms/step - loss: 0.0110 - val_loss: 0.0813\n","Epoch 106/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0109\n","Epoch 106: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 27ms/step - loss: 0.0109 - val_loss: 0.0815\n","Epoch 107/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0109\n","Epoch 107: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 27ms/step - loss: 0.0109 - val_loss: 0.0817\n","Epoch 108/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0108\n","Epoch 108: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 33ms/step - loss: 0.0108 - val_loss: 0.0819\n","Epoch 109/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0107\n","Epoch 109: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 30ms/step - loss: 0.0107 - val_loss: 0.0821\n","Epoch 110/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0106\n","Epoch 110: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 29ms/step - loss: 0.0106 - val_loss: 0.0823\n","Epoch 111/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0106\n","Epoch 111: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 27ms/step - loss: 0.0106 - val_loss: 0.0824\n","Epoch 112/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0105\n","Epoch 112: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 27ms/step - loss: 0.0105 - val_loss: 0.0826\n","Epoch 113/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0104\n","Epoch 113: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 31ms/step - loss: 0.0104 - val_loss: 0.0828\n","Epoch 114/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0104\n","Epoch 114: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 30ms/step - loss: 0.0104 - val_loss: 0.0829\n","Epoch 115/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0103\n","Epoch 115: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 24ms/step - loss: 0.0103 - val_loss: 0.0831\n","Epoch 116/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0102\n","Epoch 116: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 38ms/step - loss: 0.0102 - val_loss: 0.0832\n","Epoch 117/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0102\n","Epoch 117: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 31ms/step - loss: 0.0102 - val_loss: 0.0833\n","Epoch 118/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0101\n","Epoch 118: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 27ms/step - loss: 0.0101 - val_loss: 0.0835\n","Epoch 119/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0100\n","Epoch 119: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 40ms/step - loss: 0.0100 - val_loss: 0.0836\n","Epoch 120/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0100\n","Epoch 120: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 30ms/step - loss: 0.0100 - val_loss: 0.0837\n","Epoch 121/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0099\n","Epoch 121: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 26ms/step - loss: 0.0099 - val_loss: 0.0839\n","Epoch 122/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0099\n","Epoch 122: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 32ms/step - loss: 0.0099 - val_loss: 0.0840\n","Epoch 123/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0098\n","Epoch 123: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 34ms/step - loss: 0.0098 - val_loss: 0.0841\n","Epoch 124/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0097\n","Epoch 124: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 30ms/step - loss: 0.0097 - val_loss: 0.0842\n","Epoch 125/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0097\n","Epoch 125: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 36ms/step - loss: 0.0097 - val_loss: 0.0843\n","Epoch 126/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0096\n","Epoch 126: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 29ms/step - loss: 0.0096 - val_loss: 0.0844\n","Epoch 127/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0096\n","Epoch 127: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 30ms/step - loss: 0.0096 - val_loss: 0.0845\n","Epoch 128/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0095\n","Epoch 128: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 30ms/step - loss: 0.0095 - val_loss: 0.0846\n","Epoch 129/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0094\n","Epoch 129: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 28ms/step - loss: 0.0094 - val_loss: 0.0847\n","Epoch 130/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0094\n","Epoch 130: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 28ms/step - loss: 0.0094 - val_loss: 0.0848\n","Epoch 131/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0093\n","Epoch 131: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 26ms/step - loss: 0.0093 - val_loss: 0.0849\n","Epoch 132/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0093\n","Epoch 132: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 28ms/step - loss: 0.0093 - val_loss: 0.0850\n","Epoch 133/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0092\n","Epoch 133: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 27ms/step - loss: 0.0092 - val_loss: 0.0851\n","Epoch 134/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0092\n","Epoch 134: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 29ms/step - loss: 0.0092 - val_loss: 0.0852\n","Epoch 135/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0091\n","Epoch 135: val_loss did not improve from 0.06051\n","1/1 [==============================] - 0s 28ms/step - loss: 0.0091 - val_loss: 0.0853\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0776\n","loss_and_metrics : 0.07764878869056702\n","1/1 [==============================] - 0s 77ms/step\n"]}],"source":["import scipy\n","import numpy\n","import h5py\n","\n","#import tensorflow\n","from tensorflow import keras\n","\n","#print('scipy ' + scipy.__version__)\n","#print('numpy ' + numpy.__version__)\n","#print('h5py ' + h5py.__version__)\n","\n","#print('tensorflow ' + tensorflow.__version__)\n","#print('keras ' + keras.__version__)\n","\n","import scipy.io\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation\n","#from keras.optimizers import SGD\n","from tensorflow.keras.optimizers import Adam\n","#from keras.optimizers import Nadam\n","#from keras.optimizers import RMSprop\n","from tensorflow.keras.optimizers import Adamax\n","from tensorflow.keras.datasets import cifar10\n","#error발생: from tensorflow.keras.utils import np_utils\n","from tensorflow.keras.utils import to_categorical\n","\n","\n","train_x_data = scipy.io.loadmat('ml_detect_in_train.mat')\n","train_y_data = scipy.io.loadmat('ml_detect_out_train.mat')\n","\n","train_x = train_x_data['in']\n","train_y = train_y_data['out']\n","\n","\n","\n","val_x_data = scipy.io.loadmat('ml_detect_in_val.mat')\n","val_y_data = scipy.io.loadmat('ml_detect_out_val.mat')\n","\n","val_x = val_x_data['in']\n","val_y = val_y_data['out']\n","\n","\n","# relu, tanh, elu, selu\n","\n","model = Sequential()\n","model.add(Dense(units=100, input_dim=40, activation=\"relu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"relu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"relu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"relu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"relu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=4, activation=\"linear\", kernel_initializer='normal'))\n","\n","\n","#model.compile(loss='mean_squared_error', optimizer='adam')\n","model.compile(loss='mean_squared_error', optimizer='adamax')\n","\n","#model.fit(train_x, train_y, epochs=1000, batch_size=32)\n","\n","from keras.callbacks import EarlyStopping\n","from keras.callbacks import ModelCheckpoint\n","early_stopping = EarlyStopping(patience = 100) # 조기종료 콜백함수 정의, 100 에포크 동안은 기다림\n","checkpoint_callback = ModelCheckpoint('hl5_0100.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n","model.fit(train_x, train_y, epochs=3000, batch_size=32, validation_data=(val_x, val_y), callbacks=[early_stopping, checkpoint_callback])\n","\n","\n","from keras.models import load_model\n","model_cp = load_model('hl5_0100.h5')\n","\n","test_x_data = scipy.io.loadmat('ml_detect_in_test.mat')\n","test_y_data = scipy.io.loadmat('ml_detect_out_test.mat')\n","test_x = test_x_data['in']\n","test_y = test_y_data['out']\n","\n","loss_and_metrics = model_cp.evaluate(test_x, test_y, batch_size=32)\n","\n","print('loss_and_metrics : ' + str(loss_and_metrics))\n","\n","\n","yhat=model_cp.predict(test_x)\n","scipy.io.savemat('hl5_0500_pred.mat',dict([('predict_ch', yhat) ]))"]}]}