{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN2MZJVIZxq9h0P6bBgK7bC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IiCm7eLMrF6Q","executionInfo":{"status":"ok","timestamp":1694325813600,"user_tz":-540,"elapsed":13392,"user":{"displayName":"최미금","userId":"03270121767541003919"}},"outputId":"50f79f17-4ba2-46b0-c747-2fcf0418c226"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3666\n","Epoch 1: val_loss improved from inf to 0.34913, saving model to hl5_0100.h5\n","1/1 [==============================] - 3s 3s/step - loss: 0.3666 - val_loss: 0.3491\n","Epoch 2/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3626\n","Epoch 2: val_loss improved from 0.34913 to 0.34578, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 107ms/step - loss: 0.3626 - val_loss: 0.3458\n","Epoch 3/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3590"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 3: val_loss improved from 0.34578 to 0.34212, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 101ms/step - loss: 0.3590 - val_loss: 0.3421\n","Epoch 4/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3549\n","Epoch 4: val_loss improved from 0.34212 to 0.33798, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.3549 - val_loss: 0.3380\n","Epoch 5/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3504\n","Epoch 5: val_loss improved from 0.33798 to 0.33340, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.3504 - val_loss: 0.3334\n","Epoch 6/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3453\n","Epoch 6: val_loss improved from 0.33340 to 0.32832, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 105ms/step - loss: 0.3453 - val_loss: 0.3283\n","Epoch 7/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3397\n","Epoch 7: val_loss improved from 0.32832 to 0.32268, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.3397 - val_loss: 0.3227\n","Epoch 8/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3334\n","Epoch 8: val_loss improved from 0.32268 to 0.31640, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.3334 - val_loss: 0.3164\n","Epoch 9/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3264\n","Epoch 9: val_loss improved from 0.31640 to 0.30942, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.3264 - val_loss: 0.3094\n","Epoch 10/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3186\n","Epoch 10: val_loss improved from 0.30942 to 0.30164, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.3186 - val_loss: 0.3016\n","Epoch 11/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3098\n","Epoch 11: val_loss improved from 0.30164 to 0.29297, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.3098 - val_loss: 0.2930\n","Epoch 12/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3001\n","Epoch 12: val_loss improved from 0.29297 to 0.28327, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.3001 - val_loss: 0.2833\n","Epoch 13/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2891\n","Epoch 13: val_loss improved from 0.28327 to 0.27247, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.2891 - val_loss: 0.2725\n","Epoch 14/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2769\n","Epoch 14: val_loss improved from 0.27247 to 0.26050, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.2769 - val_loss: 0.2605\n","Epoch 15/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2633\n","Epoch 15: val_loss improved from 0.26050 to 0.24723, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.2633 - val_loss: 0.2472\n","Epoch 16/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2481\n","Epoch 16: val_loss improved from 0.24723 to 0.23251, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.2481 - val_loss: 0.2325\n","Epoch 17/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2313\n","Epoch 17: val_loss improved from 0.23251 to 0.21639, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.2313 - val_loss: 0.2164\n","Epoch 18/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2128\n","Epoch 18: val_loss improved from 0.21639 to 0.19883, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.2128 - val_loss: 0.1988\n","Epoch 19/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1925\n","Epoch 19: val_loss improved from 0.19883 to 0.17991, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.1925 - val_loss: 0.1799\n","Epoch 20/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1705\n","Epoch 20: val_loss improved from 0.17991 to 0.15983, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.1705 - val_loss: 0.1598\n","Epoch 21/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1470\n","Epoch 21: val_loss improved from 0.15983 to 0.13913, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.1470 - val_loss: 0.1391\n","Epoch 22/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1225\n","Epoch 22: val_loss improved from 0.13913 to 0.11932, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1225 - val_loss: 0.1193\n","Epoch 23/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0986\n","Epoch 23: val_loss improved from 0.11932 to 0.10326, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0986 - val_loss: 0.1033\n","Epoch 24/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0783\n","Epoch 24: val_loss improved from 0.10326 to 0.09498, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0783 - val_loss: 0.0950\n","Epoch 25/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0663\n","Epoch 25: val_loss did not improve from 0.09498\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0663 - val_loss: 0.0975\n","Epoch 26/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0660\n","Epoch 26: val_loss did not improve from 0.09498\n","1/1 [==============================] - 0s 42ms/step - loss: 0.0660 - val_loss: 0.1084\n","Epoch 27/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0754\n","Epoch 27: val_loss did not improve from 0.09498\n","1/1 [==============================] - 0s 38ms/step - loss: 0.0754 - val_loss: 0.1186\n","Epoch 28/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0847\n","Epoch 28: val_loss did not improve from 0.09498\n","1/1 [==============================] - 0s 37ms/step - loss: 0.0847 - val_loss: 0.1209\n","Epoch 29/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0861\n","Epoch 29: val_loss did not improve from 0.09498\n","1/1 [==============================] - 0s 41ms/step - loss: 0.0861 - val_loss: 0.1163\n","Epoch 30/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0805\n","Epoch 30: val_loss did not improve from 0.09498\n","1/1 [==============================] - 0s 42ms/step - loss: 0.0805 - val_loss: 0.1067\n","Epoch 31/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0698\n","Epoch 31: val_loss did not improve from 0.09498\n","1/1 [==============================] - 0s 41ms/step - loss: 0.0698 - val_loss: 0.0952\n","Epoch 32/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0573\n","Epoch 32: val_loss improved from 0.09498 to 0.08496, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0573 - val_loss: 0.0850\n","Epoch 33/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0462\n","Epoch 33: val_loss improved from 0.08496 to 0.07740, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0462 - val_loss: 0.0774\n","Epoch 34/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0381\n","Epoch 34: val_loss improved from 0.07740 to 0.07280, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0381 - val_loss: 0.0728\n","Epoch 35/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0333\n","Epoch 35: val_loss improved from 0.07280 to 0.07064, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0333 - val_loss: 0.0706\n","Epoch 36/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0310\n","Epoch 36: val_loss improved from 0.07064 to 0.07007, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0310 - val_loss: 0.0701\n","Epoch 37/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 37: val_loss did not improve from 0.07007\n","1/1 [==============================] - 0s 42ms/step - loss: 0.0304 - val_loss: 0.0703\n","Epoch 38/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0305\n","Epoch 38: val_loss did not improve from 0.07007\n","1/1 [==============================] - 0s 39ms/step - loss: 0.0305 - val_loss: 0.0709\n","Epoch 39/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0307\n","Epoch 39: val_loss did not improve from 0.07007\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0307 - val_loss: 0.0713\n","Epoch 40/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0306\n","Epoch 40: val_loss did not improve from 0.07007\n","1/1 [==============================] - 0s 39ms/step - loss: 0.0306 - val_loss: 0.0715\n","Epoch 41/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 41: val_loss did not improve from 0.07007\n","1/1 [==============================] - 0s 42ms/step - loss: 0.0300 - val_loss: 0.0712\n","Epoch 42/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 42: val_loss did not improve from 0.07007\n","1/1 [==============================] - 0s 39ms/step - loss: 0.0289 - val_loss: 0.0707\n","Epoch 43/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0273\n","Epoch 43: val_loss did not improve from 0.07007\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0273 - val_loss: 0.0701\n","Epoch 44/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0254\n","Epoch 44: val_loss improved from 0.07007 to 0.06948, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0254 - val_loss: 0.0695\n","Epoch 45/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0234\n","Epoch 45: val_loss improved from 0.06948 to 0.06909, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0234 - val_loss: 0.0691\n","Epoch 46/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0217\n","Epoch 46: val_loss improved from 0.06909 to 0.06908, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0217 - val_loss: 0.0691\n","Epoch 47/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0204\n","Epoch 47: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 39ms/step - loss: 0.0204 - val_loss: 0.0695\n","Epoch 48/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0196\n","Epoch 48: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0196 - val_loss: 0.0704\n","Epoch 49/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0195\n","Epoch 49: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0195 - val_loss: 0.0717\n","Epoch 50/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0197\n","Epoch 50: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 40ms/step - loss: 0.0197 - val_loss: 0.0730\n","Epoch 51/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0203\n","Epoch 51: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 42ms/step - loss: 0.0203 - val_loss: 0.0743\n","Epoch 52/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0208\n","Epoch 52: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 39ms/step - loss: 0.0208 - val_loss: 0.0753\n","Epoch 53/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0211\n","Epoch 53: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 41ms/step - loss: 0.0211 - val_loss: 0.0759\n","Epoch 54/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0210\n","Epoch 54: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 42ms/step - loss: 0.0210 - val_loss: 0.0761\n","Epoch 55/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0206\n","Epoch 55: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0206 - val_loss: 0.0760\n","Epoch 56/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0199\n","Epoch 56: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0199 - val_loss: 0.0758\n","Epoch 57/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0191\n","Epoch 57: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 42ms/step - loss: 0.0191 - val_loss: 0.0756\n","Epoch 58/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0183\n","Epoch 58: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0183 - val_loss: 0.0755\n","Epoch 59/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0177\n","Epoch 59: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 37ms/step - loss: 0.0177 - val_loss: 0.0756\n","Epoch 60/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0173\n","Epoch 60: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 38ms/step - loss: 0.0173 - val_loss: 0.0758\n","Epoch 61/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0170\n","Epoch 61: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0170 - val_loss: 0.0761\n","Epoch 62/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0169\n","Epoch 62: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0169 - val_loss: 0.0765\n","Epoch 63/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0168\n","Epoch 63: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0168 - val_loss: 0.0769\n","Epoch 64/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0166\n","Epoch 64: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0166 - val_loss: 0.0773\n","Epoch 65/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0165\n","Epoch 65: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 39ms/step - loss: 0.0165 - val_loss: 0.0777\n","Epoch 66/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0162\n","Epoch 66: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 41ms/step - loss: 0.0162 - val_loss: 0.0781\n","Epoch 67/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0159\n","Epoch 67: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 38ms/step - loss: 0.0159 - val_loss: 0.0784\n","Epoch 68/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0155\n","Epoch 68: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 40ms/step - loss: 0.0155 - val_loss: 0.0788\n","Epoch 69/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0152\n","Epoch 69: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 41ms/step - loss: 0.0152 - val_loss: 0.0792\n","Epoch 70/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0148\n","Epoch 70: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 42ms/step - loss: 0.0148 - val_loss: 0.0796\n","Epoch 71/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0145\n","Epoch 71: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0145 - val_loss: 0.0801\n","Epoch 72/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0142\n","Epoch 72: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 41ms/step - loss: 0.0142 - val_loss: 0.0806\n","Epoch 73/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0140\n","Epoch 73: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0140 - val_loss: 0.0811\n","Epoch 74/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0139\n","Epoch 74: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 40ms/step - loss: 0.0139 - val_loss: 0.0816\n","Epoch 75/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0138\n","Epoch 75: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0138 - val_loss: 0.0821\n","Epoch 76/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0137\n","Epoch 76: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 39ms/step - loss: 0.0137 - val_loss: 0.0825\n","Epoch 77/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0136\n","Epoch 77: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 40ms/step - loss: 0.0136 - val_loss: 0.0829\n","Epoch 78/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0134\n","Epoch 78: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0134 - val_loss: 0.0833\n","Epoch 79/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0133\n","Epoch 79: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0133 - val_loss: 0.0836\n","Epoch 80/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0131\n","Epoch 80: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0131 - val_loss: 0.0839\n","Epoch 81/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0129\n","Epoch 81: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 40ms/step - loss: 0.0129 - val_loss: 0.0841\n","Epoch 82/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0128\n","Epoch 82: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 42ms/step - loss: 0.0128 - val_loss: 0.0844\n","Epoch 83/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0126\n","Epoch 83: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0126 - val_loss: 0.0847\n","Epoch 84/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0125\n","Epoch 84: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 42ms/step - loss: 0.0125 - val_loss: 0.0850\n","Epoch 85/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0124\n","Epoch 85: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 41ms/step - loss: 0.0124 - val_loss: 0.0854\n","Epoch 86/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0123\n","Epoch 86: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 40ms/step - loss: 0.0123 - val_loss: 0.0857\n","Epoch 87/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0122\n","Epoch 87: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 40ms/step - loss: 0.0122 - val_loss: 0.0861\n","Epoch 88/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0121\n","Epoch 88: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 42ms/step - loss: 0.0121 - val_loss: 0.0864\n","Epoch 89/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0120\n","Epoch 89: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 42ms/step - loss: 0.0120 - val_loss: 0.0868\n","Epoch 90/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0119\n","Epoch 90: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 42ms/step - loss: 0.0119 - val_loss: 0.0871\n","Epoch 91/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0118\n","Epoch 91: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 42ms/step - loss: 0.0118 - val_loss: 0.0875\n","Epoch 92/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0117\n","Epoch 92: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 40ms/step - loss: 0.0117 - val_loss: 0.0879\n","Epoch 93/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0116\n","Epoch 93: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0116 - val_loss: 0.0882\n","Epoch 94/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0115\n","Epoch 94: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 42ms/step - loss: 0.0115 - val_loss: 0.0886\n","Epoch 95/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0114\n","Epoch 95: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 38ms/step - loss: 0.0114 - val_loss: 0.0889\n","Epoch 96/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0113\n","Epoch 96: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 39ms/step - loss: 0.0113 - val_loss: 0.0892\n","Epoch 97/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0112\n","Epoch 97: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 39ms/step - loss: 0.0112 - val_loss: 0.0895\n","Epoch 98/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0112\n","Epoch 98: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 38ms/step - loss: 0.0112 - val_loss: 0.0897\n","Epoch 99/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0111\n","Epoch 99: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 39ms/step - loss: 0.0111 - val_loss: 0.0900\n","Epoch 100/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0110\n","Epoch 100: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 39ms/step - loss: 0.0110 - val_loss: 0.0902\n","Epoch 101/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0109\n","Epoch 101: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 42ms/step - loss: 0.0109 - val_loss: 0.0904\n","Epoch 102/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0109\n","Epoch 102: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 39ms/step - loss: 0.0109 - val_loss: 0.0907\n","Epoch 103/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0108\n","Epoch 103: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0108 - val_loss: 0.0909\n","Epoch 104/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0107\n","Epoch 104: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 41ms/step - loss: 0.0107 - val_loss: 0.0911\n","Epoch 105/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0107\n","Epoch 105: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 39ms/step - loss: 0.0107 - val_loss: 0.0913\n","Epoch 106/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0106\n","Epoch 106: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 40ms/step - loss: 0.0106 - val_loss: 0.0915\n","Epoch 107/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0105\n","Epoch 107: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0105 - val_loss: 0.0917\n","Epoch 108/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0105\n","Epoch 108: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0105 - val_loss: 0.0919\n","Epoch 109/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0104\n","Epoch 109: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 41ms/step - loss: 0.0104 - val_loss: 0.0922\n","Epoch 110/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0103\n","Epoch 110: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0103 - val_loss: 0.0924\n","Epoch 111/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0103\n","Epoch 111: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 40ms/step - loss: 0.0103 - val_loss: 0.0926\n","Epoch 112/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0102\n","Epoch 112: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0102 - val_loss: 0.0928\n","Epoch 113/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0102\n","Epoch 113: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0102 - val_loss: 0.0930\n","Epoch 114/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0101\n","Epoch 114: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0101 - val_loss: 0.0932\n","Epoch 115/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0101\n","Epoch 115: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0101 - val_loss: 0.0933\n","Epoch 116/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0100\n","Epoch 116: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 40ms/step - loss: 0.0100 - val_loss: 0.0935\n","Epoch 117/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0100\n","Epoch 117: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 40ms/step - loss: 0.0100 - val_loss: 0.0936\n","Epoch 118/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0099\n","Epoch 118: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0099 - val_loss: 0.0938\n","Epoch 119/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0099\n","Epoch 119: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 39ms/step - loss: 0.0099 - val_loss: 0.0939\n","Epoch 120/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0098\n","Epoch 120: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 40ms/step - loss: 0.0098 - val_loss: 0.0941\n","Epoch 121/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0098\n","Epoch 121: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0098 - val_loss: 0.0942\n","Epoch 122/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0097\n","Epoch 122: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0097 - val_loss: 0.0944\n","Epoch 123/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0096\n","Epoch 123: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 39ms/step - loss: 0.0096 - val_loss: 0.0945\n","Epoch 124/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0096\n","Epoch 124: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0096 - val_loss: 0.0947\n","Epoch 125/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0095\n","Epoch 125: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0095 - val_loss: 0.0948\n","Epoch 126/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0095\n","Epoch 126: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 41ms/step - loss: 0.0095 - val_loss: 0.0950\n","Epoch 127/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0094\n","Epoch 127: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 41ms/step - loss: 0.0094 - val_loss: 0.0951\n","Epoch 128/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0094\n","Epoch 128: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 40ms/step - loss: 0.0094 - val_loss: 0.0952\n","Epoch 129/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0093\n","Epoch 129: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0093 - val_loss: 0.0953\n","Epoch 130/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0093\n","Epoch 130: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0093 - val_loss: 0.0954\n","Epoch 131/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0092\n","Epoch 131: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0092 - val_loss: 0.0955\n","Epoch 132/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0092\n","Epoch 132: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0092 - val_loss: 0.0957\n","Epoch 133/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0091\n","Epoch 133: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0091 - val_loss: 0.0958\n","Epoch 134/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0091\n","Epoch 134: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0091 - val_loss: 0.0959\n","Epoch 135/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0090\n","Epoch 135: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0090 - val_loss: 0.0960\n","Epoch 136/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0090\n","Epoch 136: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0090 - val_loss: 0.0961\n","Epoch 137/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0089\n","Epoch 137: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0089 - val_loss: 0.0961\n","Epoch 138/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0089\n","Epoch 138: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0089 - val_loss: 0.0963\n","Epoch 139/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0088\n","Epoch 139: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0088 - val_loss: 0.0964\n","Epoch 140/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0088\n","Epoch 140: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0088 - val_loss: 0.0965\n","Epoch 141/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0087\n","Epoch 141: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0087 - val_loss: 0.0966\n","Epoch 142/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0087\n","Epoch 142: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0087 - val_loss: 0.0966\n","Epoch 143/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0086\n","Epoch 143: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0086 - val_loss: 0.0967\n","Epoch 144/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0086\n","Epoch 144: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 127ms/step - loss: 0.0086 - val_loss: 0.0968\n","Epoch 145/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0085\n","Epoch 145: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0085 - val_loss: 0.0969\n","Epoch 146/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0085\n","Epoch 146: val_loss did not improve from 0.06908\n","1/1 [==============================] - 0s 102ms/step - loss: 0.0085 - val_loss: 0.0970\n","1/1 [==============================] - 0s 250ms/step - loss: 0.0808\n","loss_and_metrics : 0.08075924217700958\n","1/1 [==============================] - 0s 212ms/step\n"]}],"source":["import scipy\n","import numpy\n","import h5py\n","\n","#import tensorflow\n","from tensorflow import keras\n","\n","#print('scipy ' + scipy.__version__)\n","#print('numpy ' + numpy.__version__)\n","#print('h5py ' + h5py.__version__)\n","\n","#print('tensorflow ' + tensorflow.__version__)\n","#print('keras ' + keras.__version__)\n","\n","import scipy.io\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation\n","#from keras.optimizers import SGD\n","from tensorflow.keras.optimizers import Adam\n","#from keras.optimizers import Nadam\n","#from keras.optimizers import RMSprop\n","from tensorflow.keras.optimizers import Adamax\n","from tensorflow.keras.datasets import cifar10\n","#error발생: from tensorflow.keras.utils import np_utils\n","from tensorflow.keras.utils import to_categorical\n","\n","\n","train_x_data = scipy.io.loadmat('ml_detect_in_train.mat')\n","train_y_data = scipy.io.loadmat('ml_detect_out_train.mat')\n","\n","train_x = train_x_data['in']\n","train_y = train_y_data['out']\n","\n","\n","\n","val_x_data = scipy.io.loadmat('ml_detect_in_val.mat')\n","val_y_data = scipy.io.loadmat('ml_detect_out_val.mat')\n","\n","val_x = val_x_data['in']\n","val_y = val_y_data['out']\n","\n","\n","# relu, tanh, elu, selu\n","\n","model = Sequential()\n","model.add(Dense(units=100, input_dim=40, activation=\"relu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"relu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"relu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"relu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"relu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=4, activation=\"linear\", kernel_initializer='normal'))\n","\n","\n","#model.compile(loss='mean_squared_error', optimizer='adam')\n","model.compile(loss='mean_squared_error', optimizer='adamax')\n","\n","#model.fit(train_x, train_y, epochs=1000, batch_size=32)\n","\n","from keras.callbacks import EarlyStopping\n","from keras.callbacks import ModelCheckpoint\n","early_stopping = EarlyStopping(patience = 100) # 조기종료 콜백함수 정의, 100 에포크 동안은 기다림\n","checkpoint_callback = ModelCheckpoint('hl5_0100.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n","model.fit(train_x, train_y, epochs=3000, batch_size=32, validation_data=(val_x, val_y), callbacks=[early_stopping, checkpoint_callback])\n","\n","\n","from keras.models import load_model\n","model_cp = load_model('hl5_0100.h5')\n","\n","test_x_data = scipy.io.loadmat('ml_detect_in_test.mat')\n","test_y_data = scipy.io.loadmat('ml_detect_out_test.mat')\n","test_x = test_x_data['in']\n","test_y = test_y_data['out']\n","\n","loss_and_metrics = model_cp.evaluate(test_x, test_y, batch_size=32)\n","\n","print('loss_and_metrics : ' + str(loss_and_metrics))\n","\n","\n","yhat=model_cp.predict(test_x)\n","scipy.io.savemat('hl5_0500_pred.mat',dict([('predict_ch', yhat) ]))"]}]}